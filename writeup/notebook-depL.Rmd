---
title: Exploring Word Order Predictions of Theories of Communicatively Efficient Language Production
author: Michael Hahn
date: March 2018
output:
  html_document:
    toc: true
---



<style>
img {
    max-width: 300%;

    /* other options:
    max-width: none;
    max-width: 700px;
    max-width: 9in;
    max-width: 25cm;
    etc
    */
}
</style>



```{r, echo=FALSE}


weighted.sd = function(vec, weight) {
  if(length(vec) == 1) {
     return(NA)
  }
  return(sqrt(weighted.mean(vec*vec, weight, na.rm=TRUE) - weighted.mean(vec, weight, na.rm=TRUE)**2))
}

languages = c('Hebrew', 'Romanian', 'Finnish', 'Danish', 'Old_Church_Slavonic', 'Galician-TreeGal', 'Swedish-LinES', 'Marathi', 'Greek', 'Latin-PROIEL', 'Polish', 'Spanish-AnCora', 'Finnish-FTB', 'Kazakh', 'Arabic', 'Japanese', 'Slovenian', 'Ancient_Greek-PROIEL', 'Latvian', 'Swedish_Sign_Language', 'Coptic', 'Turkish', 'Ancient_Greek', 'Ukrainian', 'Hungarian', 'Russian-SynTagRus', 'Italian-ParTUT', 'Chinese', 'Dutch-LassySmall', 'Italian', 'Bulgarian', 'Irish', 'Romanian-Nonstandard', 'Norwegian-Nynorsk', 'Indonesian', 'Latin-ITTB', 'Tamil', 'French-Sequoia', 'Belarusian', 'Lithuanian', 'Afrikaans', 'Persian', 'Portuguese-BR', 'Croatian', 'Russian', 'English-ParTUT', 'Arabic-NYUAD', 'Estonian', 'Gothic', 'Telugu', 'Czech-CLTT', 'Catalan', 'Dutch', 'French-FTB', 'Spanish', 'English', 'French', 'Galician', 'Slovenian-SST', 'Korean', 'Portuguese', 'Basque', 'German', 'Urdu', 'Hindi', 'Slovak', 'Czech-CAC', 'Italian-PoSTWITA', 'Latin', 'Swedish', 'Vietnamese', 'French-ParTUT', 'Czech', 'Norwegian-Bokmaal', 'North_Sami', 'English-LinES', 'Serbian', 'Czech-FicTree')

library(dplyr)
library(tidyr)
library(ggplot2)



options(width=130) 

auto = read.csv("CS_SCR/deps/manual_output/auto-summary.tsv", sep="\t")# %>% rename(Quality=AverageLength)
auto$Direction = NA
auto$FileName = as.character(auto$FileName)
data = read.csv("CS_SCR/deps/manual_output/results.tsv", sep="\t") %>% rename(AverageLoss=Perplexity)
data$DH_Weight = NA
data$Counter = NA
data$Language = "English"
#data$Objective = NA
data$FileName = as.character(data$FileName)
#data$ObjectiveName = NA
data$EntropyWeight = NA
data$LR_POLICY=NA
data$Lagrange_Lambda=NA
data$Lagrange_B=NA
data$L2_Weight=NA
data = bind_rows(data, auto)


#aggregate(data["Counter"], by=c(data["Language"], data["FileName"], data["ModelName"], data["ObjectiveName"]), NROW)

# important that Counter is never NA

data = data %>% filter(Counter > 10)
################################

forAll = c("readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorpora.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopes.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVarRateUID.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVar.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVarDepLBugfix.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVarRateUIDDepLBugfix.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVarRateUIDLogDepL.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaLogVarDepLBugfix.py")

data = data %>% mutate(RegType = ifelse(ModelName == "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaSurp.py", "Surp", ifelse(ModelName %in% c("readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaDepL.py", "readDataRegressionDepLengthAndSurprisalRandomEffectsVariationalUIDRandInterceptSlopesAllCorporaDepLOnly_Bugfix.py"), "DepL", ifelse(ModelName %in% forAll, "All", ifelse(ModelName == "readDataRegressionDepLengthAndSurprisalRandomEffectsVariational.py", "All_NoSlope", "NONE")))))

cat("Take into account (1) log variances, (2) UID rate vs UID")
data = data %>% mutate(Var_Slope_Surp_POS = ifelse(is.na(Var_Slope_Surp_POS), exp(LogVar_Slope_Surp_POS), Var_Slope_Surp_POS))
data = data %>% mutate(Var_Slope_Surp_Word = ifelse(is.na(Var_Slope_Surp_Word), exp(LogVar_Slope_Surp_Word), Var_Slope_Surp_Word))
data$Var_Slope_UIDRate = NA
data = data %>% mutate(Var_Slope_UIDRate = ifelse(is.na(Var_Slope_UIDRate), exp(LogVar_Slope_UIDRate), Var_Slope_UIDRate))
data = data %>% mutate(Var_Slope_UID = ifelse(is.na(Var_Slope_UID), exp(LogVar_Slope_UID), Var_Slope_UID))
data = data %>% mutate(DH_Sigma = ifelse(is.na(DH_Sigma), exp(DH_LogSigma), DH_Sigma))
data = data %>% mutate(Mean_Slope_DepLength = ifelse(is.na(Mean_Slope_DepLength), exp(Mean_Slope_DepLogLength), Mean_Slope_DepLength))
data = data %>% mutate(Distance_Sigma = ifelse(is.na(Distance_Sigma), exp(Distance_LogSigma), Distance_Sigma))
data = data %>% mutate(Var_Slope_DepLength = ifelse(is.na(Var_Slope_DepLength), exp(LogVar_Slope_DepLogLength), Var_Slope_DepLength))
data = data %>% mutate(Var_Slope_DepLength = ifelse(is.na(Var_Slope_DepLength), exp(LogVar_Slope_DepLength), Var_Slope_DepLength))

cat(" TODO do same with other variances and Sigmas")
cat("ALSO take into account depLogLength")
cat("Where is Norwegian surprisal data???")



```





```{r, echo=FALSE}
freqs = data.frame()
for(lang in languages) {
   freqs = rbind(freqs, read.csv(paste("CS_SCR/deps/",lang,"-stats.tsv",sep=""), sep="\t") %>% mutate(Language=lang))
}
# TODO this is a hacky way to ensure Norwegian is in the data
freqs$Language[freqs$Language=="Norwegian-Nynorsk"] = "Norwegian"
data = merge(data, freqs %>% select(Head, Dependency, Dependent, Count, Language), by=c("Head", "Dependency", "Dependent", "Language")) #, all=TRUE)
######################################
realWeights = data %>% filter(ObjectiveName == "variational") %>% select(Head, Dependency, Dependent, Language, DH_Mean, DH_Sigma, Distance_Mean, Distance_Sigma)%>% rename(Real_DH_Mean = DH_Mean, Real_DH_Sigma = DH_Sigma, Real_Distance_Mean = Distance_Mean, Real_Distance_Sigma = Distance_Sigma)
data = merge(data, realWeights, by=c("Language", "Head", "Dependency", "Dependent"), all=TRUE)



# TODO here, need to make sure that only ONE variational model is taken per language



######################################
data = data %>% mutate(DH = ifelse(is.na(DH_Weight), Direction == "DH", DH_Weight>0))
data = data %>% mutate(Language = as.factor(Language), Head = as.factor(Head), Dependency = as.factor(Dependency), Dependent = as.factor(Dependent))
```



```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeads = function(dataset) {
    for(language in unique(dataset$Language)) {
        dodge = position_dodge(.9)
        agr = dataset %>% filter(Language==language) %>% group_by(Language, Head) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
        plot = ggplot(agr, aes(x=Head,y=DistanceWeight, fill=Real_DistanceWeight)) +
          geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
          geom_bar(stat="identity", position=dodge) +
          facet_wrap(~Language)
        print(plot)
    }
}
```


```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeadsAveragedOverLangs = function(dataset) {
    dodge = position_dodge(.9)
    agr = dataset %>% group_by(Head) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
    plot = ggplot(agr, aes(x=Head,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge)
    print(plot)
}
```


```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossHeads = function(dataset) {
  for(language in unique(dataset$Language)) {
      dodge = position_dodge(.9)
      agr = dataset %>% filter(Language==language) %>% group_by(Language,Head) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE))
      plot = ggplot(agr, aes(x=Head,y=DH_Weight, fill=Real_DH_Weight)) +
        geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
        geom_bar(stat="identity", position=dodge) +
        facet_wrap(~Language)
      print(plot)
  }
}
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependents = function(dataset) {
for(language in unique(dataset$Language)) {
    dodge = position_dodge(.9)
    agr = dataset %>% filter(Language==language) %>% group_by(Language, Dependent) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
    plot = ggplot(agr, aes(x=Dependent,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) +
      facet_wrap(~Language)
    print(plot)
}
}
```



```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependentsAveragedOverLanguages = function(dataset) {
    dodge = position_dodge(.9)
    agr = dataset  %>% group_by(Dependent) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
    plot = ggplot(agr, aes(x=Dependent,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge)
    print(plot)
}
```

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependents = function(dataset) {
  for(language in unique(dataset$Language)) {
      dodge = position_dodge(.9)
      agr = dataset %>% filter(Language==language) %>% group_by(Language,Dependent) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE))
      plot = ggplot(agr, aes(x=Dependent,y=DH_Weight, fill=Real_DH_Weight)) +
        geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
        geom_bar(stat="identity", position=dodge) +
        facet_wrap(~Language)
      print(plot)
  }
}
```


```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependencies = function(dataset) {

for(language in unique(dataset$Language)) {
    dodge = position_dodge(.9)
    agr = dataset %>% filter(Language==language) %>% group_by(Language, Dependency) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
    plot = ggplot(agr, aes(x=Dependency,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) +
      facet_wrap(~Language)
    print(plot)

    
    agr = dataset %>% filter(Language==language) %>% mutate(Direction = ifelse(DH_Weight > 0, "DH", "HD")) %>% group_by(Language, Direction, Dependency) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE))
    plot = ggplot(agr, aes(x=Dependency,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) +
      facet_wrap(~Language+Direction)
    print(plot)
}
}
```

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged = function(dataset) {
    dodge = position_dodge(.9)
    agr = dataset %>% group_by(Dependency) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE), Count = sum(Count)) %>% filter(Count >= median(Count))
    plot = ggplot(agr, aes(x=Dependency,y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) 
    print(plot)
}
```


```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged2 = function(dataset) {
    dodge = position_dodge(.9)
    agr = dataset %>% group_by(Head,Dependency,Dependent) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE), Count = sum(Count))
    agr = agr[order(-agr$Count),] 
    agr = agr[(1:50),]
    plot = ggplot(agr, aes(x=paste(Head,Dependency,Dependent,sep="\n"),y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge)
    print(plot)
}
```



```{r, echo=FALSE, fig.width=35, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged3 = function(dataset) {
    dodge = position_dodge(.9)
    dataset = dataset %>% mutate(Direction = ifelse(DH_Weight > 0, "DH", "HD"))
    agr = dataset %>% group_by(Direction,Head,Dependency,Dependent) %>% summarise(DistanceWeight_SD = weighted.sd(DistanceWeight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Real_DistanceWeight = weighted.mean(Real_Distance_Mean, Count, na.rm=TRUE), Count = sum(Count))
    agr = agr[order(-agr$Count),] 
    agr = agr[(1:50),]
    plot = ggplot(agr, aes(x=paste(Head,Dependency,Dependent,sep="\n"),y=DistanceWeight, fill=Real_DistanceWeight)) +
      geom_errorbar(aes(ymin=DistanceWeight-DistanceWeight_SD, ymax=DistanceWeight+DistanceWeight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) +
      facet_wrap( ~ Direction)
    print(plot)
}
```
























```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependencies = function(dataset) {
  for(language in unique(dataset$Language)) {
      dodge = position_dodge(.9)
      agr = dataset %>% filter(Language==language) %>% group_by(Language,Dependency) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE))
      plot = ggplot(agr, aes(x=Dependency,y=DH_Weight, fill=Real_DH_Weight)) +
        geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
        geom_bar(stat="identity", position=dodge) +
        facet_wrap(~Language)
      print(plot)
  }
}
```



```{r, echo=FALSE, fig.width=35, fig.height=5}
generalOrderPredictionsAcrossDependenciesAveraged = function(dataset) {
    dodge = position_dodge(.9)
    agr = dataset %>% group_by(Dependency) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE), Count = sum(Count)) %>% filter(Count >= quantile(Count, 0.7))
    plot = ggplot(agr, aes(x=Dependency,y=DH_Weight, fill=Real_DH_Weight)) +
      geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge)
    print(plot)

    dodge = position_dodge(.9)
    agr = dataset %>% group_by(Head,Dependency,Dependent) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE), Count = sum(Count))
    agr = agr[order(-agr$Count),] 
    agr = agr[(1:50),]
    plot = ggplot(agr, aes(x=paste(Head,Dependency,Dependent,sep="\n"),y=DH_Weight, fill=Real_DH_Weight)) +
      geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge)
    print(plot)

    dodge = position_dodge(.9)
    dataset = dataset %>% mutate(Direction = ifelse(DH_Weight > 0, "DH", "HD"))
    agr = dataset %>% group_by(Direction,Head,Dependency,Dependent) %>% summarise(DH_Weight_SD = weighted.sd(DH_Weight, Count), DH_Weight = weighted.mean(DH_Weight, Count), Real_DH_Weight = weighted.mean(Real_DH_Mean, Count, na.rm=TRUE), Count = sum(Count))
    agr = agr[order(-agr$Count),] 
    agr = agr[(1:50),]
    plot = ggplot(agr, aes(x=paste(Head,Dependency,Dependent,sep="\n"),y=DH_Weight, fill=Real_DH_Weight)) +
      geom_errorbar(aes(ymin=DH_Weight-DH_Weight_SD, ymax=DH_Weight+DH_Weight_SD), position=dodge, width=.25) +
      geom_bar(stat="identity", position=dodge) +
      facet_wrap( ~ Direction)
    print(plot)
}
```







# Minimizing Dependency Length

```{r, echo=FALSE}
dataDepL = data[data$Objective == "DepL",]
depLDistances = as.data.frame(dataDepL %>% group_by(Dependency) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))

depLDistancesDetails = as.data.frame(dataDepL %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count, na.rm=TRUE), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count, na.rm=TRUE)))

depLPerHead = as.data.frame(dataDepL %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))

depLNP = depLPerHead %>% filter(Head == "NOUN") #%>% filter(Count > 1000)
depL_G_20 = depLNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
#depL_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#depL_G_20[order(depL_G_20$Language, depL_G_20$DH, depL_G_20$DistanceWeight),]

depL_G_20 = depL_G_20 %>% mutate(DH = DH_Weight>0)
```


## General Distance Predictions across Heads

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeads(dataDepL)
```

### averaged over languages

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeadsAveragedOverLangs(dataDepL)
```

## General Order Predictions across Heads

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossHeads(dataDepL)
```

## General Distance Predictions across Dependents

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependents(dataDepL)
```

### averaged over languages

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependentsAveragedOverLanguages(dataDepL)
```

## General Order Predictions across Dependents

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependents(dataDepL)
```

## General Distance Predictions across Dependencies

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependencies(dataDepL)
```

### averaged across languages

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged(dataDepL)
```

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged2(dataDepL)
```

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged3(dataDepL)
```

## General Order Predictions across Dependencies

```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependencies(dataDepL)
```


### averaged over Languages


```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependenciesAveraged(dataDepL)
```





## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(depL_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)



library(data.table)
depL_G_20 = dcast(setDT(depL_G_20), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))




##########

depLCoreDependents = depLPerHead %>% filter(Head == "VERB") %>% filter(Count > 1000)
depL_CoreDependents = depLCoreDependents %>% filter(Dependency %in% c("obj", "iobj", "nsubj", "nsubjpass"))

# distance
depL_CoreDependents %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#depL_CoreDependents[order(depL_CoreDependents$Language, depL_CoreDependents$DH, depL_CoreDependents$DistanceWeight),]

```

## Distance logits for Core Arguments
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(depL_CoreDependents, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)



library(data.table)
depL_CoreDependents = dcast(setDT(depL_CoreDependents), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"), na.rm=TRUE)


#################################

# nominal vs pronominal core arguments

#lmPerHeadDep = as.data.frame(dataLM %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))


```

## Distance logits for Nominal and Pronominal Arguments
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}

depLNounPronArgs = dataDepL %>% filter(Head == "VERB", Dependent %in% c("NOUN", "PRON")) %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))

depL_NounPronArgs = depLNounPronArgs %>% filter(Dependency %in% c("obj", "iobj", "nsubj", "nsubjpass"))

plot = ggplot(depL_NounPronArgs, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency + Dependent) 
print(plot)



library(data.table)
depL_NounPronArgs = dcast(setDT(depL_NounPronArgs), Language +FileName ~ Dependency + Dependent, value.var=c("DistanceWeight", "DH_Weight"), na.rm=TRUE)



# prepositions vs postpositions
prePostpos = dataDepL %>% filter(Head == "NOUN", Dependency == "case", Dependent == "ADP") %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))


library(data.table)
prePostpos = dcast(setDT(prePostpos), Language +FileName ~ 1, value.var=c("DistanceWeight", "DH_Weight"), na.rm=TRUE)

prePostposByLan = prePostpos %>% group_by(Language) %>% summarise(DH_Weight_SD = sd(DH_Weight, na.rm=TRUE), DH_Weight = mean(DH_Weight, na.rm=TRUE), DistanceWeight = mean(DistanceWeight, na.rm=TRUE))
# cor(prePostposByLan$DH_Weight, prePostposByLan$DistanceWeight)
# `when before the head, bind more strongly'

#################################











#dataDepL = data[data$ModelName == "readDataDistEnglishGPUDepLength.py",]
#depLDistances = as.data.frame(dataDepL %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))

#dataEnglish = data[data$ModelName == "inferWeightsForEnglish.py",]
#englishDistances = as.data.frame(dataEnglish %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))


#data = merge(data, dataEnglish %>% rename(DistanceWeightEnglish=DistanceWeight,DH_WeightEnglish = DH_Weight) %>% select(Head, Dependency, Dependent, DistanceWeightEnglish, DH_WeightEnglish), by=c("Head", "Dependency", "Dependent"))

# [1] "Head"           "Dependency"     "Dependent"      "FileName"      
# [5] "ModelName"      "Quality"        "Direction"      "DistanceWeight"
# [9] "DH_Weight"      "Counter"        "Count"         


```

# Optimizing UID

```{r, echo=FALSE}


#dataUID = data[data$ModelName == "readDataDistEnglishGPUFreeUID.py",]
dataUID = data[data$Objective == "UID",]
dataUID = dataUID %>% filter(ModelName %in% c("readDataDistEnglishGPUFreeUIDCorrectMean.py", "readDataDistEnglishGPUFreeUIDCorrectMeanAllCorpora.py"))
uidDistances = as.data.frame(dataUID %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))


uidDistancesDetails = as.data.frame(dataUID %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


uidPerHead = as.data.frame(dataUID %>% group_by(Head,Dependency,  Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


uidNP = uidPerHead %>% filter(Head == "NOUN") %>% filter(Count > 1000)
uid_G_20 = uidNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
#uid_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#uid_G_20[order(uid_G_20$Language, uid_G_20$DH, uid_G_20$DistanceWeight),]

uid_G_20 = uid_G_20 %>% mutate(DH = DH_Weight>0)

```

## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(uid_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)




library(data.table)
uid_G_20 = dcast(setDT(uid_G_20), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))



#dataUID = data[data$ModelName == "readDataDistEnglishGPUFreeUID.py",]
dataUIDRate = data[data$ObjectiveName == "UIDRate",]
uidRateDistances = as.data.frame(dataUIDRate %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))


uidRateDistancesDetails = as.data.frame(dataUIDRate %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


uidRatePerHead = as.data.frame(dataUIDRate %>% group_by(Head,Dependency,  Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


uidRateNP = uidRatePerHead %>% filter(Head == "NOUN") %>% filter(Count > 1000)
uidRate_G_20 = uidRateNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
uidRate_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
uidRate_G_20[order(uidRate_G_20$Language, uidRate_G_20$DH, uidRate_G_20$DistanceWeight),]

uidRate_G_20 = uidRate_G_20 %>% mutate(DH = DH_Weight>0)

```

## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(uidRate_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)




library(data.table)
uidRate_G_20 = dcast(setDT(uidRate_G_20), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))


```

# Optimizing Online Assignment of Dependency Labels (~Syntactic Roles)

```{r, echo=FALSE}




# TODO depLabels
dataDepLabels = data[data$Objective == "depLabels",]
depLabelsDistances = as.data.frame(dataDepLabels %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))

depLabelsDistancesDetails = as.data.frame(dataDepLabels %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


depLabelsPerHead = as.data.frame(dataDepLabels %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))


depLabelsNP = depLabelsPerHead %>% filter(Head == "NOUN") #%>% filter(Count > 1000)
depLabels_G_20 = depLabelsNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
depLabels_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
depLabels_G_20[order(depLabels_G_20$Language, depLabels_G_20$DH, depLabels_G_20$DistanceWeight),]

depLabels_G_20 = depLabels_G_20 #%>% mutate(DH = DH_Weight>0)


```







## General Distance Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeads(dataDepLabels)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeadsAveragedOverLangs(dataDepLabels)
```

## General Order Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossHeads(dataDepLabels)
```

## General Distance Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependents(dataDepLabels)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependentsAveragedOverLanguages(dataDepLabels)
```

## General Order Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependents(dataDepLabels)
```

## General Distance Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependencies(dataDepLabels)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged(dataDepLabels)
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged2(dataDepLabels)
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged3(dataDepLabels)
```

## General Order Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependencies(dataDepLabels)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependenciesAveraged(dataDepLabels)
```







## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(depLabels_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)





library(data.table)
depLabels_G_20 = dcast(setDT(depLabels_G_20), Language +FileName + AverageLoss ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))



```

# Optimizing Surprisal (Sum of POS surprisal and Word Surprisal)


```{r, echo=FALSE}
dataLM = data[data$Objective == "LM",]
lmDistances = as.data.frame(dataLM %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))

lmDistancesDetails = as.data.frame(dataLM %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))

lmPerHead = as.data.frame(dataLM %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))

lmNP = lmPerHead %>% filter(Head == "NOUN") #%>% filter(Count > 1000)
lm_G_20 = lmNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
lm_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#lm_G_20[order(lm_G_20$Language, lm_G_20$DH, lm_G_20$DistanceWeight),]

lm_G_20 = lm_G_20 #%>% mutate(DH = DH_Weight>0)

```

## General Distance Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeads(dataLM)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeadsAveragedOverLangs(dataLM)
```

## General Order Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossHeads(dataLM)
```

## General Distance Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependents(dataLM)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependentsAveragedOverLanguages(dataLM)
```

## General Order Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependents(dataLM)
```

## General Distance Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependencies(dataLM)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged(dataLM)
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged2(dataLM)
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged3(dataLM)
```

## General Order Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependencies(dataLM)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependenciesAveraged(dataLM)
```

## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(lm_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)

```

## Distance logits for Det, Num, Adj, by word order
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
lm_G_20 = lm_G_20 %>% mutate(DH_Binary = ifelse(DH > 0, "DH", "HD"))
plot = ggplot(lm_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency + DH_Binary) 
print(plot)

library(data.table)
lm_G_20 = dcast(setDT(lm_G_20), Language +FileName + AverageLoss ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))


dataLMLex = dataLM %>% filter(ModelName %in% c("readDataDistCrossGPUFreeMomentumEarlyStopEntropyPersevereAnnealAllCorporaOnlyLex.py"))
lmLexDistances = as.data.frame(dataLMLex %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))

lmLexDistancesDetails = as.data.frame(dataLMLex %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


lmLexPerHead = as.data.frame(dataLMLex %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))


lmLexNP = lmLexPerHead %>% filter(Head == "NOUN") #%>% filter(Count > 1000)
lmLex_G_20 = lmLexNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
lmLex_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#lmLex_G_20[order(lmLex_G_20$Language, lmLex_G_20$DH, lmLex_G_20$DistanceWeight),]

lmLex_G_20 = lmLex_G_20 #%>% mutate(DH = DH_Weight>0)

```

## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(lmLex_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)


```

## Distance logits for Det, Num, Adj, by word order
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
lmLex_G_20 = lmLex_G_20 %>% mutate(DH_Binary = ifelse(DH > 0, "DH", "HD"))
plot = ggplot(lmLex_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency + DH_Binary) 
print(plot)






library(data.table)
lmLex_G_20 = dcast(setDT(lmLex_G_20), Language +FileName + AverageLoss ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))

#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_amod > 0,]$DistanceWeight_amod)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_amod < 0,]$DistanceWeight_amod)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_det < 0,]$DistanceWeight_det)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_det > 0,]$DistanceWeight_det)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_num > 0,]$DistanceWeight_num)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_num > 0,]$DistanceWeight_num)
#> summary(lmLex_G_20[lmLex_G_20$DH_Weight_num < 0,]$DistanceWeight_num)


```

## Distance logits for Det, Num, Adj, by word order
Here, including runs where I forced Det, Num, Adj to appear after the noun.

```{r, echo=FALSE}


dataLMLexForceHD = dataLM %>% filter(ModelName %in% c("readDataDistCrossGPUFreeMomentumEarlyStopEntropyPersevereAnnealAllCorporaOnlyLex.py", "readDataDistCrossGPUFreeMomentumEarlyStopEntropyPersevereAnnealAllCorporaOnlyLexRestrNP.py"))
lmLexForceHDDistances = as.data.frame(dataLMLexForceHD %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))

lmLexForceHDDistancesDetails = as.data.frame(dataLMLexForceHD %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


lmLexForceHDPerHead = as.data.frame(dataLMLexForceHD %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))


lmLexForceHDNP = lmLexForceHDPerHead %>% filter(Head == "NOUN") #%>% filter(Count > 1000)
lmLexForceHD_G_20 = lmLexForceHDNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
lmLexForceHD_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
#lmLexForceHD_G_20[order(lmLexForceHD_G_20$Language, lmLexForceHD_G_20$DH, lmLexForceHD_G_20$DistanceWeight),]

lmLexForceHD_G_20 = lmLexForceHD_G_20 #%>% mutate(DH = DH_Weight>0)


```


## Distance logits for Det, Num, Adj
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
plot = ggplot(lmLexForceHD_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency) 
print(plot)


```

## Distance logits for Det, Num, Adj, by word order
More negative values indicate stronger preference to be close to the noun.

```{r, echo=FALSE}
lmLexForceHD_G_20 = lmLexForceHD_G_20 %>% mutate(DH_Binary = ifelse(DH > 0, "DH", "HD"))
plot = ggplot(lmLexForceHD_G_20, aes(DistanceWeight, fill=Language)) +
  geom_histogram(binwidth=2) +
  facet_wrap( ~ Dependency + DH_Binary) 
print(plot)






library(data.table)
lmLexForceHD_G_20 = dcast(setDT(lmLexForceHD_G_20), Language +FileName + AverageLoss ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))

#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_amod > 0,]$DistanceWeight_amod)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_amod < 0,]$DistanceWeight_amod)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_det < 0,]$DistanceWeight_det)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_det > 0,]$DistanceWeight_det)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_num > 0,]$DistanceWeight_num)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_num > 0,]$DistanceWeight_num)
#> summary(lmLexForceHD_G_20[lmLexForceHD_G_20$DH_Weight_num < 0,]$DistanceWeight_num)


```


```{r}



##############

lmCoreDependents = lmPerHead %>% filter(Head == "VERB") %>% filter(Count > 1000)
lm_CoreDependents = lmCoreDependents %>% filter(Dependency %in% c("obj", "iobj", "nsubj", "nsubjpass"))

# distance
lm_CoreDependents %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
lm_CoreDependents[order(lm_CoreDependents$Language, lm_CoreDependents$DH, lm_CoreDependents$DistanceWeight),]


library(data.table)
lm_CoreDependents = dcast(setDT(lm_CoreDependents), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH", "DH_Weight"), na.rm=TRUE)

###################

# Left-Right Asymmetry
#summary(lmer(DistanceWeight ~ DH_Weight + (1|ModelName) + (1|FileName) + (1|Head) + (1|Dependency) + (1|Dependent), data=dataLM))

# entropy regularization masks this effect
#summary(lmer(DistanceWeight ~ DH_Weight*EntropyWeight + (1|FileName) + (1|Head) + (1|Dependency) + (1|Dependent), data=dataLM))

###################

# nominal vs pronominal core arguments

#lmPerHeadDep = as.data.frame(dataLM %>% group_by(Head,Dependency,  Language, FileName, AverageLoss) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count)))


lmNounPronArgs = dataLM %>% filter(Head == "VERB", Dependent %in% c("NOUN", "PRON")) %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))

lm_NounPronArgs = lmNounPronArgs %>% filter(Dependency %in% c("obj", "iobj", "nsubj", "nsubjpass"))

library(data.table)
lm_NounPronArgs = dcast(setDT(lm_NounPronArgs), Language +FileName ~ Dependency + Dependent, value.var=c("DistanceWeight", "DH_Weight"), na.rm=TRUE)


######## 
# 7: SOV (/+OSV) => Adv before Verb
lmG7_1 = dataLM %>% filter(Head == "VERB", Dependent == "ADV", Dependency == "advmod") %>% select(Language, FileName, DH_Weight) %>% rename(DH_advmod = DH_Weight)
lmG7_2 = dataLM %>% filter(Head == "VERB", Dependent == "NOUN", Dependency == "obj") %>% select(Language, FileName, DH_Weight) %>% rename(DH_obj = DH_Weight)
lmG7 = merge(lmG7_1, lmG7_2, by=c("Language", "FileName"))
cor.test(lmG7$DH_advmod, lmG7$DH_obj)
# there seems to be nothing here


# 13. "If the nominal object always precedes the verb, then verb forms subordinate to the main verb also precede it."

# 16. "In languages with dominant order VSO, an inflected auxiliary always precedes the main verb. In languages with dominant order SOV, an inflected auxiliary always follows the main verb."


# sounds like dependency length

# 17. "With overwhelmingly more than chance frequency, languages with dominant order VSO have the adjective after the noun." [but see Dryer 1992]

# sounds like dependency length

# 18. "When the descriptive adjective precedes the noun, the demonstrative and the numeral, with overwhelmingly more than chance frequency, do likewise."

summary(lm_G_20 %>% filter(DH_Weight_amod > 0))
# seems potentially opposite to prediction

# 21. "If some or all adverbs follow the adjective they modify, then the language is one in which the qualifying adjective follows the noun and the verb precedes its nominal object as the dominant order."

lmG21_1 = dataLM %>% filter(Head == "ADJ", Dependent == "ADV", Dependency == "advmod") %>% select(Language, FileName, DH_Weight) %>% rename(DH_advmod = DH_Weight)
lmG21_2 = dataLM %>% filter(Head == "NOUN", Dependent == "ADJ", Dependency == "amod") %>% select(Language, FileName, DH_Weight)%>% rename(DH_amod = DH_Weight)
lmG21_3 = dataLM %>% filter(Head == "VERB", Dependent == "NOUN", Dependency == "obj") %>% select(Language, FileName, DH_Weight)%>% rename(DH_obj = DH_Weight)
lmG21 = merge(lmG21_1, lmG21_2, by=c("Language", "FileName"))
lmG21 = merge(lmG21,   lmG21_3, by=c("Language", "FileName"))
summary(lmG21[lmG21$DH_advmod < 0,])
# there seems to be nothing here (sounds more like dependency length)

#Head == "VERB", Dependent %in% c("NOUN", "PRON")) %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))



# 23. "If in apposition the proper noun usually precedes the common noun, then the language is one in which the governing noun precedes its dependent genitive. With much better than chance frequency, if the common noun usually precedes the proper noun, the dependent genitive precedes its governing noun."

# 24. "If the relative expression precedes the noun either as the only construction or as an alternate construction, either the language is postpositional, or the adjective precedes the noun or both."




##############################
# 25. "If the pronominal object follows the verb, so does the nominal object."
lmG25 = dataLM %>% filter(Head == "VERB", Dependent %in% c("NOUN", "PRON")) %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))

lmG25 = lmG25 %>% filter(Dependency %in% c("obj"))

lmG25 = dcast(setDT(lmG25), Language +FileName ~ Dependency + Dependent, value.var=c("DH_Weight"), na.rm=TRUE)

summary(lmG25 %>% filter(obj_PRON < 0))
# expect obj_NOUN to be negative, but there is nothing





# prepositions vs postpositions


prePostpos = dataLM %>% filter(Head == "NOUN", Dependency == "case", Dependent == "ADP") %>%group_by(Dependency, Dependent, Language, FileName, AverageLoss)  %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count, na.rm=TRUE), DistanceWeight = weighted.mean(DistanceWeight, Count), DH = weighted.mean(DH, Count, na.rm=TRUE), Count=sum(Count))


library(data.table)
prePostpos = dcast(setDT(prePostpos), Language +FileName ~ 1, value.var=c("DistanceWeight", "DH_Weight"), na.rm=TRUE)

prePostposByLan = prePostpos %>% group_by(Language) %>% summarise(DH_Weight_SD = sd(DH_Weight, na.rm=TRUE), DH_Weight = mean(DH_Weight, na.rm=TRUE), DistanceWeight = mean(DistanceWeight, na.rm=TRUE))
# cor(prePostposByLan$DH_Weight, prePostposByLan$DistanceWeight)
# `when before the head, bind more strongly'

#################################



```

# Optimizing Parsability

```{r, echo=FALSE}



#dataparsing = data[data$ModelName == "readDataDistEnglishGPUFreeparsing.py",]
dataparsing = data[data$Objective == "parsing",]
parsingDistances = as.data.frame(dataparsing %>% group_by(Dependency) %>% summarise(DistanceWeight = weighted.mean(DistanceWeight, Count)))


parsingDistancesDetails = as.data.frame(dataparsing %>% group_by(Dependency, Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


parsingPerHead = as.data.frame(dataparsing %>% group_by(Head,Dependency,  Language, FileName) %>% summarise(DH_Weight = weighted.mean(DH_Weight, Count), DistanceWeight = weighted.mean(DistanceWeight, Count), Count=sum(Count)))


parsingNP = parsingPerHead %>% filter(Head == "NOUN") %>% filter(Count > 1000)
parsing_G_20 = parsingNP %>% filter(Dependency %in% c("amod", "det", "nummod"))

# distance
parsing_G_20 %>% group_by(Dependency) %>% summarise(DistanceWeight = mean(DistanceWeight))

# look at existing patterns
parsing_G_20[order(parsing_G_20$Language, parsing_G_20$DH, parsing_G_20$DistanceWeight),]

parsing_G_20 = parsing_G_20 %>% mutate(DH = DH_Weight>0)

library(data.table)
parsing_G_20 = dcast(setDT(parsing_G_20), Language +FileName ~ Dependency, value.var=c("DistanceWeight", "DH_Weight"))
```

## General Distance Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeads(dataparsing)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossHeadsAveragedOverLangs(dataparsing)
```

## General Order Predictions across Heads
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossHeads(dataparsing)
```

## General Distance Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependents(dataparsing)
```

### averaged over languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependentsAveragedOverLanguages(dataparsing)
```

## General Order Predictions across Dependents
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependents(dataparsing)
```

## General Distance Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependencies(dataparsing)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged(dataparsing)
```
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged2(dataparsing)
```
```{r, echo=FALSE, fig.width=35, fig.height=5}
generalDistancePredictionsAcrossDependenciesAveraged3(dataparsing)
```

## General Order Predictions across Dependencies
```{r, echo=FALSE, fig.width=25, fig.height=5}
generalOrderPredictionsAcrossDependencies(dataparsing)
```

### averaged over Languages
```{r, echo=FALSE, fig.width=35, fig.height=5}
generalOrderPredictionsAcrossDependenciesAveraged(dataparsing)
```

